# Оценка качества и Безопасность
Для создания надежной и предсказуемой AI-системы мы реализовали собственные, легковесные модули для оценки качества и модерации контента. Такой подход избавляет нас от тяжелых внешних зависимостей и дает полный контроль над процессом.

### 1. Собственный фреймворк Оценки (`app/evaluation`)
Вместо интеграции громоздких библиотек типа `deepeval`, мы создали `CustomEvaluator`. Этот модуль использует LLM для выполнения объективной оценки ответов агента.

**Как это работает:**
-   Мы подаем на вход `CustomEvaluator` три вещи: фактический ответ агента, "идеальный" (эталонный) ответ и критерий оценки (например, "Корректность" или "Следование инструкциям").
-   Evaluator формирует специальный промпт для LLM, прося его выступить в роли беспристрастного судьи и выставить оценку по шкале от 1 до 5.
-   LLM возвращает JSON с оценкой и кратким обоснованием.

Это позволяет нам создавать гибкие тесты для проверки производительности агентов на конкретных задачах, не привязываясь к сложным внешним инструментам.

### 2. Собственные "Ограждения" (Guardrails) (`app/safety`)
Аналогично, вместо `guardrails-ai` мы реализовали `CustomGuardrailManager` для контроля за безопасностью ответов.

**Как это работает:**
-   В менеджере определен список запрещенных тем (например, финансовые, медицинские советы, разжигание ненависти).
-   Перед тем, как отдать финальный ответ пользователю, агент передает его в `CustomGuardrailManager`.
-   Менеджер с помощью LLM определяет, не нарушает ли ответ заданные правила.
-   Если ответ безопасен, он передается пользователю. Если нет — ответ блокируется, и пользователь получает стандартное сообщение о том, что тема не может быть обсуждена.

Такой подход обеспечивает базовый, но надежный уровень безопасности и контроля над контентом, генерируемым системой.

---
[<-- Назад к README](../README.md) 