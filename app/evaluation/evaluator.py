# app/evaluation/evaluator.py
import os
import json
from typing import Dict, Any

from openai import OpenAI

EVALUATION_SYSTEM_PROMPT = """
You are an impartial AI evaluator. Your task is to assess the quality of an AI agent's response based on a given criterion and a reference answer.

You must follow these instructions strictly:
1.  Compare the "Actual Answer" to the "Reference Answer".
2.  Evaluate the answer ONLY on the provided "Evaluation Criterion".
3.  Provide a score from 1 to 5, where 1 is the worst and 5 is the best.
4.  Provide a brief, single-sentence justification for your score.
5.  Your final output MUST be a JSON object with two keys: "score" (int) and "justification" (str).

Example:
Actual Answer: "The capital of France is Paris."
Reference Answer: "Paris is the capital of France."
Evaluation Criterion: "Correctness"

Your output:
{
  "score": 5,
  "justification": "The answer is factually correct and directly answers the question."
}
"""

class CustomEvaluator:
    """
    A simple, custom evaluator that uses an LLM to score an agent's response
    against a reference answer based on a specific criterion.
    """

    def __init__(self, api_key: str = None):
        api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OpenAI API key is required.")
        self.client = OpenAI(api_key=api_key)

    def evaluate(
        self,
        actual_answer: str,
        reference_answer: str,
        criterion: str,
        model: str = "gpt-4o-mini",
    ) -> Dict[str, Any]:
        """
        Evaluates the agent's response.

        Args:
            actual_answer: The answer generated by the agent.
            reference_answer: The "golden" or expected answer.
            criterion: The specific aspect to evaluate (e.g., "Correctness", "Clarity").
            model: The LLM to use for evaluation.

        Returns:
            A dictionary containing the score and justification.
        """
        if not all([actual_answer, reference_answer, criterion]):
            raise ValueError("actual_answer, reference_answer, and criterion cannot be empty.")

        user_prompt = f"""
        Actual Answer: "{actual_answer}"
        Reference Answer: "{reference_answer}"
        Evaluation Criterion: "{criterion}"
        """

        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": EVALUATION_SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                response_format={"type": "json_object"},
                temperature=0.0,
            )
            evaluation_result = json.loads(response.choices[0].message.content)
            
            if "score" not in evaluation_result or "justification" not in evaluation_result:
                raise KeyError("Evaluation response is missing 'score' or 'justification'.")

            return evaluation_result

        except KeyError as e:
            print(f"Error processing evaluation keys: {e}")
            return {"score": 0, "justification": f"Evaluation response is missing 'score' or 'justification'."}
        except json.JSONDecodeError as e:
            print(f"Error parsing evaluation response: {e}")
            return {"score": 0, "justification": "Failed to parse the evaluation response from the LLM."}
        except Exception as e:
            print(f"An unexpected error occurred during evaluation: {e}")
            return {"score": 0, "justification": "An unexpected error occurred."} 