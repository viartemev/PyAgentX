# app/safety/custom_guardrails.py
import os
import json
from typing import List, Dict, Any, Optional

from openai import OpenAI

GUARDRAIL_SYSTEM_PROMPT = """
You are a content safety moderator. Your task is to determine if a given text violates any of the provided safety policies.

You must follow these instructions strictly:
1.  Read the "Agent's Answer" and the "Forbidden Topics".
2.  Determine if the answer discusses any of the forbidden topics.
3.  Your final output MUST be a JSON object with two keys:
    - "is_safe" (boolean): `true` if the answer is safe, `false` if it violates a policy.
    - "reason" (string): A brief, single-sentence justification for your decision. If the answer is safe, the reason should be "The answer is compliant with all policies."

Example 1:
Agent's Answer: "To build a simple web server, you can use Python's Flask library."
Forbidden Topics: ["Financial advice", "Medical advice", "Illegal activities"]

Your output:
{
  "is_safe": true,
  "reason": "The answer is compliant with all policies."
}

Example 2:
Agent's Answer: "You should consider investing all your money in the latest cryptocurrency."
Forbidden Topics: ["Financial advice", "Medical advice", "Illegal activities"]

Your output:
{
  "is_safe": false,
  "reason": "The answer provides financial advice, which is a forbidden topic."
}
"""

DEFAULT_FORBIDDEN_TOPICS = [
    "Financial advice",
    "Medical advice",
    "Illegal activities or promotion of dangerous acts",
    "Hate speech or harassment",
    "Explicit or adult content",
]

class CustomGuardrailManager:
    """
    A simple, custom guardrail that uses an LLM to check if an agent's
    response violates a set of predefined policies.
    """

    def __init__(self, api_key: str = None, forbidden_topics: Optional[List[str]] = None):
        api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OpenAI API key is required.")
        self.client = OpenAI(api_key=api_key)
        self.forbidden_topics = forbidden_topics or DEFAULT_FORBIDDEN_TOPICS

    def moderate(self, answer: str, model: str = "gpt-4o-mini") -> Dict[str, Any]:
        """
        Moderates the agent's final answer.

        Args:
            answer: The final answer generated by the agent.
            model: The LLM to use for moderation.

        Returns:
            A dictionary containing the safety check result.
        """
        user_prompt = f"""
        Agent's Answer: "{answer}"
        Forbidden Topics: {json.dumps(self.forbidden_topics)}
        """

        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": GUARDRAIL_SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                response_format={"type": "json_object"},
                temperature=0.0,
            )
            moderation_result = json.loads(response.choices[0].message.content)

            if "is_safe" not in moderation_result or "reason" not in moderation_result:
                raise KeyError("Moderation response is missing 'is_safe' or 'reason'.")

            return moderation_result

        except (json.JSONDecodeError, KeyError) as e:
            print(f"Error parsing moderation response: {e}")
            # Fail safe: if we can't parse the response, assume it's not safe.
            return {"is_safe": False, "reason": "Failed to parse the moderation response from the LLM."}
        except Exception as e:
            print(f"An unexpected error occurred during moderation: {e}")
            return {"is_safe": False, "reason": "An unexpected error occurred."}

    def validate_and_format_response(self, final_answer: str) -> str:
        """
        Checks the final answer against guardrails and returns it if safe,
        or a fallback message if not.
        """
        moderation_result = self.moderate(final_answer)
        if moderation_result.get("is_safe", False):
            return final_answer
        else:
            reason = moderation_result.get("reason", "an unknown policy.")
            print(f"Blocked response due to: {reason}")
            return "I am sorry, but I cannot provide a response on this topic. It violates my safety guidelines." 